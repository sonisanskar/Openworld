{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(name):  ##function for reading the file and changing columnnames\n",
    "    df=pd.read_json(name)\n",
    "    return df.rename(columns={0:\"expr\",1:'intent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_train=read_json('is_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_test=read_json('is_test.json')\n",
    "df_is_val=read_json('is_val.json')\n",
    "df_oos_train=read_json('oos_train.json')\n",
    "df_oos_test=read_json('oos_test.json')\n",
    "df_oos_val=read_json('oos_val.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def clean_data(text):  #function to remove punctuatuions, tokenize the text and lower the text and remove stopwords\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)  #no pinctations\n",
    "    tokens = word_tokenize(text_nopunct)                       #convert into tokens\n",
    "    lower_tokens=[x.lower() for x in tokens]                     #lowercase the tokens\n",
    "    stoplist = stopwords.words('english')\n",
    "    tokens_without_sw=[word for word in lower_tokens if not word in stopwords.words()]\n",
    "    text_final=' '.join(tokens_without_sw) \n",
    "    return text_final\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_train=df_is_train.iloc[:5000,:]   #right now training data set to only 50 classes so 100*50 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_train['text_Final'] = df_is_train['expr'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expr</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how would you say fly in italian</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what's the spanish word for pasta</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how would they say butter in zambia</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how do you say fast in spanish</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what's the word for trees in norway</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>what percentage of species display cold bloode...</td>\n",
       "      <td>oos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>what does it mean to be an alpha male</td>\n",
       "      <td>oos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>what animals have alpha males</td>\n",
       "      <td>oos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>why do males want to be alpha</td>\n",
       "      <td>oos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>what's the average battery life of an android ...</td>\n",
       "      <td>oos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 expr     intent\n",
       "0                    how would you say fly in italian  translate\n",
       "1                   what's the spanish word for pasta  translate\n",
       "2                 how would they say butter in zambia  translate\n",
       "3                      how do you say fast in spanish  translate\n",
       "4                 what's the word for trees in norway  translate\n",
       "..                                                ...        ...\n",
       "95  what percentage of species display cold bloode...        oos\n",
       "96              what does it mean to be an alpha male        oos\n",
       "97                      what animals have alpha males        oos\n",
       "98                      why do males want to be alpha        oos\n",
       "99  what's the average battery life of an android ...        oos\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_is_test=df_is_test.iloc[:1500,:]\n",
    "df_is_test=df_is_test.append(df_oos_train)\n",
    "df_is_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_test['text_Final'] = df_is_test['expr'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens(text):\n",
    "    return word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_train['tokens']=df_is_train['text_Final'].apply(lambda x: make_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_test['tokens']=df_is_test['text_Final'].apply(lambda x: make_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names=df_is_train['intent'].unique()\n",
    "new=pd.get_dummies(df_is_train['intent'])\n",
    "y_train=new[label_names].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19530 words total, with a vocabulary size of 2353\n",
      "Max sentence length is 14\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in df_is_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in df_is_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2353 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH=14 #keeping max lenth as 6 as maximum of the sentences complete till 12 length\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(df_is_train['text_Final'].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(df_is_train['text_Final'].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH)##trining data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6050 words total, with a vocabulary size of 1361\n",
      "Max sentence length is 12\n"
     ]
    }
   ],
   "source": [
    "all_testing_words = [word for tokens in df_is_test[\"tokens\"] for word in tokens]\n",
    "testing_sentence_lengths = [len(tokens) for tokens in df_is_test[\"tokens\"]]\n",
    "TESTING_VOCAB = sorted(list(set(all_testing_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_testing_words), len(TESTING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(testing_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH=28\n",
    "test_sequences = tokenizer.texts_to_sequences(df_is_test[\"text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)#test_data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2354, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
    " EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    " train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "##yoon kim model not considered\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=True, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=250, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    #pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.1)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.001)(pool)\n",
    "    x = Flatten()(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 14)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 14, 300)      706200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 12, 250)      225250      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 11, 250)      300250      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 10, 250)      375250      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 4, 250)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 3, 250)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 3, 250)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 250)      0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10, 250)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2500)         0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2500)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 50)           125050      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,732,000\n",
      "Trainable params: 1,732,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/8\n",
      "4500/4500 [==============================] - ETA: 31s - loss: 0.7005 - acc: 0.48 - ETA: 18s - loss: 0.6698 - acc: 0.64 - ETA: 14s - loss: 0.6392 - acc: 0.74 - ETA: 12s - loss: 0.6125 - acc: 0.80 - ETA: 11s - loss: 0.5847 - acc: 0.83 - ETA: 9s - loss: 0.5524 - acc: 0.8605 - ETA: 9s - loss: 0.5229 - acc: 0.877 - ETA: 8s - loss: 0.4936 - acc: 0.890 - ETA: 7s - loss: 0.4642 - acc: 0.900 - ETA: 7s - loss: 0.4360 - acc: 0.908 - ETA: 6s - loss: 0.4102 - acc: 0.914 - ETA: 6s - loss: 0.3865 - acc: 0.920 - ETA: 5s - loss: 0.3661 - acc: 0.924 - ETA: 5s - loss: 0.3483 - acc: 0.928 - ETA: 5s - loss: 0.3333 - acc: 0.932 - ETA: 4s - loss: 0.3202 - acc: 0.935 - ETA: 4s - loss: 0.3094 - acc: 0.937 - ETA: 4s - loss: 0.3002 - acc: 0.940 - ETA: 4s - loss: 0.2916 - acc: 0.942 - ETA: 3s - loss: 0.2843 - acc: 0.944 - ETA: 3s - loss: 0.2778 - acc: 0.945 - ETA: 3s - loss: 0.2721 - acc: 0.947 - ETA: 2s - loss: 0.2666 - acc: 0.948 - ETA: 2s - loss: 0.2615 - acc: 0.950 - ETA: 2s - loss: 0.2567 - acc: 0.951 - ETA: 2s - loss: 0.2519 - acc: 0.952 - ETA: 1s - loss: 0.2473 - acc: 0.953 - ETA: 1s - loss: 0.2426 - acc: 0.954 - ETA: 1s - loss: 0.2384 - acc: 0.955 - ETA: 1s - loss: 0.2341 - acc: 0.956 - ETA: 0s - loss: 0.2300 - acc: 0.956 - ETA: 0s - loss: 0.2262 - acc: 0.957 - ETA: 0s - loss: 0.2226 - acc: 0.958 - ETA: 0s - loss: 0.2190 - acc: 0.958 - ETA: 0s - loss: 0.2157 - acc: 0.959 - 9s 2ms/sample - loss: 0.2152 - acc: 0.9596 - val_loss: 0.1702 - val_acc: 0.9800\n",
      "Epoch 2/8\n",
      "4500/4500 [==============================] - ETA: 7s - loss: 0.1022 - acc: 0.980 - ETA: 7s - loss: 0.1028 - acc: 0.980 - ETA: 7s - loss: 0.1032 - acc: 0.980 - ETA: 6s - loss: 0.1020 - acc: 0.980 - ETA: 6s - loss: 0.1006 - acc: 0.980 - ETA: 6s - loss: 0.1007 - acc: 0.980 - ETA: 6s - loss: 0.0997 - acc: 0.980 - ETA: 5s - loss: 0.0993 - acc: 0.980 - ETA: 5s - loss: 0.0983 - acc: 0.980 - ETA: 5s - loss: 0.0979 - acc: 0.980 - ETA: 5s - loss: 0.0973 - acc: 0.980 - ETA: 5s - loss: 0.0965 - acc: 0.980 - ETA: 5s - loss: 0.0959 - acc: 0.980 - ETA: 4s - loss: 0.0952 - acc: 0.980 - ETA: 4s - loss: 0.0947 - acc: 0.980 - ETA: 4s - loss: 0.0941 - acc: 0.980 - ETA: 4s - loss: 0.0936 - acc: 0.980 - ETA: 4s - loss: 0.0931 - acc: 0.980 - ETA: 3s - loss: 0.0925 - acc: 0.980 - ETA: 3s - loss: 0.0920 - acc: 0.980 - ETA: 3s - loss: 0.0916 - acc: 0.980 - ETA: 3s - loss: 0.0912 - acc: 0.980 - ETA: 2s - loss: 0.0908 - acc: 0.980 - ETA: 2s - loss: 0.0902 - acc: 0.980 - ETA: 2s - loss: 0.0897 - acc: 0.980 - ETA: 2s - loss: 0.0893 - acc: 0.980 - ETA: 1s - loss: 0.0889 - acc: 0.980 - ETA: 1s - loss: 0.0885 - acc: 0.980 - ETA: 1s - loss: 0.0882 - acc: 0.980 - ETA: 1s - loss: 0.0878 - acc: 0.980 - ETA: 0s - loss: 0.0874 - acc: 0.980 - ETA: 0s - loss: 0.0870 - acc: 0.980 - ETA: 0s - loss: 0.0866 - acc: 0.980 - ETA: 0s - loss: 0.0862 - acc: 0.980 - ETA: 0s - loss: 0.0859 - acc: 0.980 - 8s 2ms/sample - loss: 0.0858 - acc: 0.9803 - val_loss: 0.1969 - val_acc: 0.9800\n",
      "Epoch 3/8\n",
      "4500/4500 [==============================] - ETA: 7s - loss: 0.0702 - acc: 0.981 - ETA: 7s - loss: 0.0674 - acc: 0.981 - ETA: 6s - loss: 0.0676 - acc: 0.981 - ETA: 6s - loss: 0.0687 - acc: 0.981 - ETA: 6s - loss: 0.0687 - acc: 0.981 - ETA: 6s - loss: 0.0680 - acc: 0.981 - ETA: 6s - loss: 0.0677 - acc: 0.981 - ETA: 5s - loss: 0.0674 - acc: 0.981 - ETA: 5s - loss: 0.0671 - acc: 0.981 - ETA: 5s - loss: 0.0669 - acc: 0.981 - ETA: 5s - loss: 0.0665 - acc: 0.981 - ETA: 5s - loss: 0.0665 - acc: 0.981 - ETA: 4s - loss: 0.0662 - acc: 0.981 - ETA: 4s - loss: 0.0658 - acc: 0.981 - ETA: 4s - loss: 0.0659 - acc: 0.981 - ETA: 4s - loss: 0.0657 - acc: 0.981 - ETA: 3s - loss: 0.0652 - acc: 0.981 - ETA: 3s - loss: 0.0650 - acc: 0.981 - ETA: 3s - loss: 0.0648 - acc: 0.981 - ETA: 3s - loss: 0.0647 - acc: 0.981 - ETA: 3s - loss: 0.0643 - acc: 0.981 - ETA: 2s - loss: 0.0639 - acc: 0.981 - ETA: 2s - loss: 0.0636 - acc: 0.981 - ETA: 2s - loss: 0.0633 - acc: 0.981 - ETA: 2s - loss: 0.0629 - acc: 0.981 - ETA: 1s - loss: 0.0627 - acc: 0.981 - ETA: 1s - loss: 0.0624 - acc: 0.981 - ETA: 1s - loss: 0.0622 - acc: 0.981 - ETA: 1s - loss: 0.0618 - acc: 0.981 - ETA: 1s - loss: 0.0614 - acc: 0.982 - ETA: 0s - loss: 0.0611 - acc: 0.982 - ETA: 0s - loss: 0.0609 - acc: 0.982 - ETA: 0s - loss: 0.0604 - acc: 0.982 - ETA: 0s - loss: 0.0601 - acc: 0.982 - ETA: 0s - loss: 0.0598 - acc: 0.982 - 8s 2ms/sample - loss: 0.0598 - acc: 0.9822 - val_loss: 0.2195 - val_acc: 0.9797\n",
      "Epoch 4/8\n",
      "4500/4500 [==============================] - ETA: 8s - loss: 0.0481 - acc: 0.984 - ETA: 8s - loss: 0.0467 - acc: 0.984 - ETA: 7s - loss: 0.0461 - acc: 0.985 - ETA: 7s - loss: 0.0465 - acc: 0.985 - ETA: 6s - loss: 0.0451 - acc: 0.985 - ETA: 6s - loss: 0.0441 - acc: 0.985 - ETA: 6s - loss: 0.0440 - acc: 0.985 - ETA: 6s - loss: 0.0441 - acc: 0.985 - ETA: 5s - loss: 0.0440 - acc: 0.985 - ETA: 5s - loss: 0.0437 - acc: 0.986 - ETA: 5s - loss: 0.0436 - acc: 0.986 - ETA: 5s - loss: 0.0431 - acc: 0.986 - ETA: 5s - loss: 0.0429 - acc: 0.986 - ETA: 5s - loss: 0.0428 - acc: 0.986 - ETA: 4s - loss: 0.0425 - acc: 0.986 - ETA: 4s - loss: 0.0423 - acc: 0.986 - ETA: 4s - loss: 0.0422 - acc: 0.986 - ETA: 4s - loss: 0.0418 - acc: 0.986 - ETA: 3s - loss: 0.0415 - acc: 0.986 - ETA: 3s - loss: 0.0414 - acc: 0.986 - ETA: 3s - loss: 0.0412 - acc: 0.986 - ETA: 3s - loss: 0.0411 - acc: 0.986 - ETA: 2s - loss: 0.0408 - acc: 0.986 - ETA: 2s - loss: 0.0406 - acc: 0.986 - ETA: 2s - loss: 0.0403 - acc: 0.986 - ETA: 2s - loss: 0.0402 - acc: 0.986 - ETA: 1s - loss: 0.0400 - acc: 0.987 - ETA: 1s - loss: 0.0397 - acc: 0.987 - ETA: 1s - loss: 0.0397 - acc: 0.987 - ETA: 1s - loss: 0.0395 - acc: 0.987 - ETA: 0s - loss: 0.0393 - acc: 0.987 - ETA: 0s - loss: 0.0391 - acc: 0.987 - ETA: 0s - loss: 0.0390 - acc: 0.987 - ETA: 0s - loss: 0.0388 - acc: 0.987 - ETA: 0s - loss: 0.0384 - acc: 0.987 - 8s 2ms/sample - loss: 0.0384 - acc: 0.9875 - val_loss: 0.2371 - val_acc: 0.9790\n",
      "Epoch 5/8\n",
      "4500/4500 [==============================] - ETA: 7s - loss: 0.0316 - acc: 0.989 - ETA: 7s - loss: 0.0295 - acc: 0.990 - ETA: 7s - loss: 0.0288 - acc: 0.990 - ETA: 7s - loss: 0.0298 - acc: 0.989 - ETA: 6s - loss: 0.0285 - acc: 0.990 - ETA: 6s - loss: 0.0290 - acc: 0.990 - ETA: 6s - loss: 0.0286 - acc: 0.990 - ETA: 6s - loss: 0.0286 - acc: 0.990 - ETA: 5s - loss: 0.0284 - acc: 0.990 - ETA: 5s - loss: 0.0285 - acc: 0.990 - ETA: 5s - loss: 0.0283 - acc: 0.990 - ETA: 5s - loss: 0.0281 - acc: 0.990 - ETA: 4s - loss: 0.0280 - acc: 0.990 - ETA: 4s - loss: 0.0279 - acc: 0.990 - ETA: 4s - loss: 0.0279 - acc: 0.990 - ETA: 4s - loss: 0.0276 - acc: 0.990 - ETA: 4s - loss: 0.0274 - acc: 0.991 - ETA: 3s - loss: 0.0273 - acc: 0.991 - ETA: 3s - loss: 0.0271 - acc: 0.991 - ETA: 3s - loss: 0.0269 - acc: 0.991 - ETA: 3s - loss: 0.0268 - acc: 0.991 - ETA: 2s - loss: 0.0267 - acc: 0.991 - ETA: 2s - loss: 0.0267 - acc: 0.991 - ETA: 2s - loss: 0.0265 - acc: 0.991 - ETA: 2s - loss: 0.0264 - acc: 0.991 - ETA: 2s - loss: 0.0263 - acc: 0.991 - ETA: 1s - loss: 0.0263 - acc: 0.991 - ETA: 1s - loss: 0.0260 - acc: 0.991 - ETA: 1s - loss: 0.0259 - acc: 0.991 - ETA: 1s - loss: 0.0259 - acc: 0.991 - ETA: 0s - loss: 0.0257 - acc: 0.991 - ETA: 0s - loss: 0.0256 - acc: 0.991 - ETA: 0s - loss: 0.0255 - acc: 0.991 - ETA: 0s - loss: 0.0253 - acc: 0.991 - ETA: 0s - loss: 0.0252 - acc: 0.991 - 8s 2ms/sample - loss: 0.0252 - acc: 0.9917 - val_loss: 0.2505 - val_acc: 0.9766\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 \n",
    "num_epochs = 8\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 447us/sample\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(test_cnn_data, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ans=df_is_test['intent'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metric(y_test,original_ans):\n",
    "    output_class_pred=[]\n",
    "    y_test=y_test.tolist()\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    for i in range(len(y_test)):\n",
    "        m=max(y_test[i])\n",
    "        output_class_pred.append([label_names[y_test[i].index(m)],m])\n",
    "        \n",
    "    for t in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]: \n",
    "        for i in range(len(y_test)):\n",
    "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
    "                output_class_pred[i][0]=\"oos\"\n",
    "        \n",
    "        rightly_predicted_count=0\n",
    "        for i in range(len(y_test)):\n",
    "            if( output_class_pred[i][0]==original_ans[i]):\n",
    "                rightly_predicted_count+=1\n",
    "\n",
    "        accuracy=rightly_predicted_count/len(y_test)\n",
    "        print('accuracy',rightly_predicted_count/len(y_test))\n",
    "        TP=0\n",
    "        FP=0\n",
    "        TN=0\n",
    "        FN=0\n",
    "        for i in range(len(y_test)):\n",
    "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
    "                FP+=1\n",
    "        \n",
    "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
    "                TP+=1\n",
    "        \n",
    "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
    "                FN+=1\n",
    "        \n",
    "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
    "                TN+=1\n",
    "        precision=TP/(TP+FP)\n",
    "        recalll=TP/(FN+TP)\n",
    "        F1=2*precision*recalll/(precision+recalll)\n",
    "        sensiti=TP/(TP+FN)\n",
    "        specifici=TN/(TN+FP)\n",
    "        numerator=TP*TN - FP*FN\n",
    "    \n",
    "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "        MCc=numerator/denominator\n",
    "        G_mean1=np.sqrt(sensiti*precision)\n",
    "        G_mean2=np.sqrt(sensiti*specifici)\n",
    "        print('precision_oos:' ,TP/(TP+FP))\n",
    "        print('recall_oos:',TP/(FN+TP))\n",
    "        print(\"F1_oos:\",F1)\n",
    "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
    "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
    "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "        print(\"MCC :\",MCc)\n",
    "        acc.append(accuracy)\n",
    "        pre.append(precision)\n",
    "        recall.append(recalll)\n",
    "        f1.append(F1)\n",
    "        specificity.append(specifici)\n",
    "        sensitivity.append(sensiti)\n",
    "        GMean1.append(G_mean1)\n",
    "        Gmean2.append(G_mean2)\n",
    "        MCC.append(MCc)\n",
    "        matrix=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    data={'threshold':matrix,'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6525\n",
      "precision_oos: 0.1092436974789916\n",
      "recall_oos: 0.13\n",
      "F1_oos: 0.11872146118721462\n",
      "Specificity_oos: 0.9293333333333333\n",
      "Sensitivity_oos  0.13\n",
      "G-mean1: 0.11917080461366747\n",
      "G-mean2 0.3475821245883242\n",
      "MCC : 0.05473855985667431\n",
      "accuracy 0.62375\n",
      "precision_oos: 0.13109756097560976\n",
      "recall_oos: 0.43\n",
      "F1_oos: 0.20093457943925236\n",
      "Specificity_oos: 0.81\n",
      "Sensitivity_oos  0.43\n",
      "G-mean1: 0.23742778106092008\n",
      "G-mean2 0.5901694671871801\n",
      "MCC : 0.14390516895863842\n",
      "accuracy 0.578125\n",
      "precision_oos: 0.11764705882352941\n",
      "recall_oos: 0.68\n",
      "F1_oos: 0.20058997050147492\n",
      "Specificity_oos: 0.66\n",
      "Sensitivity_oos  0.68\n",
      "G-mean1: 0.282842712474619\n",
      "G-mean2 0.6699253689777691\n",
      "MCC : 0.17133069613002574\n",
      "accuracy 0.544375\n",
      "precision_oos: 0.13101604278074866\n",
      "recall_oos: 0.98\n",
      "F1_oos: 0.2311320754716981\n",
      "Specificity_oos: 0.5666666666666667\n",
      "Sensitivity_oos  0.98\n",
      "G-mean1: 0.358323487822294\n",
      "G-mean2 0.7452069063913279\n",
      "MCC : 0.2652147211237289\n",
      "accuracy 0.495625\n",
      "precision_oos: 0.11785714285714285\n",
      "recall_oos: 0.99\n",
      "F1_oos: 0.21063829787234042\n",
      "Specificity_oos: 0.506\n",
      "Sensitivity_oos  0.99\n",
      "G-mean1: 0.34158245187446534\n",
      "G-mean2 0.7077711494543982\n",
      "MCC : 0.2404256876421392\n",
      "accuracy 0.46\n",
      "precision_oos: 0.10760869565217392\n",
      "recall_oos: 0.99\n",
      "F1_oos: 0.1941176470588235\n",
      "Specificity_oos: 0.45266666666666666\n",
      "Sensitivity_oos  0.99\n",
      "G-mean1: 0.32639333433091455\n",
      "G-mean2 0.6694325955613455\n",
      "MCC : 0.2167574732657188\n",
      "accuracy 0.405\n",
      "precision_oos: 0.09737098344693282\n",
      "recall_oos: 1.0\n",
      "F1_oos: 0.17746228926353153\n",
      "Specificity_oos: 0.382\n",
      "Sensitivity_oos  1.0\n",
      "G-mean1: 0.3120432397071483\n",
      "G-mean2 0.6180614856144977\n",
      "MCC : 0.19286190830936092\n",
      "accuracy 0.35375\n",
      "precision_oos: 0.08952551477170993\n",
      "recall_oos: 1.0\n",
      "F1_oos: 0.16433853738701723\n",
      "Specificity_oos: 0.322\n",
      "Sensitivity_oos  1.0\n",
      "G-mean1: 0.2992081462322006\n",
      "G-mean2 0.5674504383644443\n",
      "MCC : 0.169785793741675\n",
      "accuracy 0.265625\n",
      "precision_oos: 0.07905138339920949\n",
      "recall_oos: 1.0\n",
      "F1_oos: 0.14652014652014653\n",
      "Specificity_oos: 0.22333333333333333\n",
      "Sensitivity_oos  1.0\n",
      "G-mean1: 0.2811607785577666\n",
      "G-mean2 0.47258156262526085\n",
      "MCC : 0.13287140007976428\n"
     ]
    }
   ],
   "source": [
    "resi=check_metric(y_test,original_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_all</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>specificity</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>Gmean1</th>\n",
       "      <th>Gmean2</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.118721</td>\n",
       "      <td>0.929333</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.119171</td>\n",
       "      <td>0.347582</td>\n",
       "      <td>0.054739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.623750</td>\n",
       "      <td>0.131098</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.200935</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.237428</td>\n",
       "      <td>0.590169</td>\n",
       "      <td>0.143905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.200590</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.669925</td>\n",
       "      <td>0.171331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.544375</td>\n",
       "      <td>0.131016</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.231132</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.358323</td>\n",
       "      <td>0.745207</td>\n",
       "      <td>0.265215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.495625</td>\n",
       "      <td>0.117857</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.210638</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.341582</td>\n",
       "      <td>0.707771</td>\n",
       "      <td>0.240426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.107609</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.194118</td>\n",
       "      <td>0.452667</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.326393</td>\n",
       "      <td>0.669433</td>\n",
       "      <td>0.216757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.097371</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.177462</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.312043</td>\n",
       "      <td>0.618061</td>\n",
       "      <td>0.192862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.353750</td>\n",
       "      <td>0.089526</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.164339</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.299208</td>\n",
       "      <td>0.567450</td>\n",
       "      <td>0.169786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.079051</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.146520</td>\n",
       "      <td>0.223333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.281161</td>\n",
       "      <td>0.472582</td>\n",
       "      <td>0.132871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_all  precision  recall  F1_score  specificity  \\\n",
       "0        0.1      0.652500   0.109244    0.13  0.118721     0.929333   \n",
       "1        0.2      0.623750   0.131098    0.43  0.200935     0.810000   \n",
       "2        0.3      0.578125   0.117647    0.68  0.200590     0.660000   \n",
       "3        0.4      0.544375   0.131016    0.98  0.231132     0.566667   \n",
       "4        0.5      0.495625   0.117857    0.99  0.210638     0.506000   \n",
       "5        0.6      0.460000   0.107609    0.99  0.194118     0.452667   \n",
       "6        0.7      0.405000   0.097371    1.00  0.177462     0.382000   \n",
       "7        0.8      0.353750   0.089526    1.00  0.164339     0.322000   \n",
       "8        0.9      0.265625   0.079051    1.00  0.146520     0.223333   \n",
       "\n",
       "   sensitivity    Gmean1    Gmean2       MCC  \n",
       "0         0.13  0.119171  0.347582  0.054739  \n",
       "1         0.43  0.237428  0.590169  0.143905  \n",
       "2         0.68  0.282843  0.669925  0.171331  \n",
       "3         0.98  0.358323  0.745207  0.265215  \n",
       "4         0.99  0.341582  0.707771  0.240426  \n",
       "5         0.99  0.326393  0.669433  0.216757  \n",
       "6         1.00  0.312043  0.618061  0.192862  \n",
       "7         1.00  0.299208  0.567450  0.169786  \n",
       "8         1.00  0.281161  0.472582  0.132871  "
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "resi.to_csv('results.csv', mode='w', index = False, header=res.columns,columns=res.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### the propesed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=train_embedding_weights\n",
    "max_sequence_length=MAX_SEQUENCE_LENGTH\n",
    "num_words=len(train_word_index)+1\n",
    "trainable=False\n",
    "embedding_dim=EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(num_words,embedding_dim,weights=[embeddings],input_length=max_sequence_length,trainable=trainable))\n",
    "    #model.add(Bidirectional(GRU(150, return_sequences=True,dropout=0.1,recurrent_dropout=0.1)))\n",
    "    model.add(Conv1D(\n",
    "            filters=hp.Int('conv_1_filter',min_value=16,max_value=128,step=16),\n",
    "            kernel_size=hp.Choice('conv_1_kernel',values=[3,5]),\n",
    "            activation='relu'\n",
    "        ))\n",
    "    #model.add(MaxPooling1D()),\n",
    "    model.add(Conv1D(\n",
    "            filters=hp.Int('conv_2_filter',min_value=16,max_value=128,step=16),\n",
    "            kernel_size=hp.Choice('conv_1_kernel',values=[3,5]),\n",
    "            activation='relu'  \n",
    "        ))\n",
    "    #model.add(MaxPooling1D())\n",
    "    #model.add(Conv1D(\n",
    "            #filters=hp.Int('conv_2_filter',min_value=16,max_value=128,step=16),\n",
    "            #kernel_size=hp.Choice('conv_1_kernel',values=[3,5]),\n",
    "            #activation='relu'  \n",
    "        #))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "            units=hp.Int('dense_1_units',min_value=32,max_value=128,step=16),\n",
    "            activation='relu'\n",
    "        ))\n",
    "    #model.add(Dense(\n",
    "            #units=hp.Int('dense_2_units',min_value=32,max_value=256,step=16),\n",
    "            #activation='relu'\n",
    "        #))\n",
    "    model.add(Dropout(0.1))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(units=50,activation='sigmoid'))\n",
    "    #model=Model(Input(shape=(max_sequence_length,), dtype='int32'),pred)\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate',values=[1e-2,1e-3,1e-1])),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project o5u4eu/open_world/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from o5u4eu/open_world/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner_search=RandomSearch(build_model,objective=\"val_accuracy\",max_trials=5,directory='o5u4eu',project_name='open_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_search.search(x_train,y_tr,epochs=3,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tuner_search.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 14, 300)           706200    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 12, 112)           100912    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 10, 80)            26960     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5, 80)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 112)               44912     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5650      \n",
      "=================================================================\n",
      "Total params: 884,634\n",
      "Trainable params: 178,434\n",
      "Non-trainable params: 706,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - ETA: 2:18 - loss: 0.0955 - accuracy: 0.98 - ETA: 23s - loss: 0.0944 - accuracy: 0.9801 - ETA: 14s - loss: 0.0934 - accuracy: 0.980 - ETA: 8s - loss: 0.0933 - accuracy: 0.980 - ETA: 7s - loss: 0.0932 - accuracy: 0.98 - ETA: 6s - loss: 0.0927 - accuracy: 0.98 - ETA: 5s - loss: 0.0919 - accuracy: 0.98 - ETA: 4s - loss: 0.0917 - accuracy: 0.98 - ETA: 3s - loss: 0.0913 - accuracy: 0.98 - ETA: 3s - loss: 0.0908 - accuracy: 0.98 - ETA: 2s - loss: 0.0902 - accuracy: 0.98 - ETA: 2s - loss: 0.0898 - accuracy: 0.98 - ETA: 2s - loss: 0.0890 - accuracy: 0.98 - ETA: 1s - loss: 0.0884 - accuracy: 0.98 - ETA: 1s - loss: 0.0876 - accuracy: 0.98 - ETA: 1s - loss: 0.0871 - accuracy: 0.98 - ETA: 1s - loss: 0.0865 - accuracy: 0.98 - ETA: 1s - loss: 0.0858 - accuracy: 0.98 - ETA: 1s - loss: 0.0849 - accuracy: 0.98 - ETA: 0s - loss: 0.0842 - accuracy: 0.98 - ETA: 0s - loss: 0.0836 - accuracy: 0.98 - ETA: 0s - loss: 0.0827 - accuracy: 0.98 - ETA: 0s - loss: 0.0817 - accuracy: 0.98 - ETA: 0s - loss: 0.0807 - accuracy: 0.98 - ETA: 0s - loss: 0.0798 - accuracy: 0.98 - ETA: 0s - loss: 0.0790 - accuracy: 0.98 - ETA: 0s - loss: 0.0779 - accuracy: 0.98 - 3s 577us/sample - loss: 0.0778 - accuracy: 0.9808 - val_loss: 0.1855 - val_accuracy: 0.9794\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - ETA: 1s - loss: 0.0604 - accuracy: 0.98 - ETA: 1s - loss: 0.0554 - accuracy: 0.98 - ETA: 1s - loss: 0.0542 - accuracy: 0.98 - ETA: 1s - loss: 0.0533 - accuracy: 0.98 - ETA: 1s - loss: 0.0518 - accuracy: 0.98 - ETA: 1s - loss: 0.0516 - accuracy: 0.98 - ETA: 1s - loss: 0.0509 - accuracy: 0.98 - ETA: 1s - loss: 0.0507 - accuracy: 0.98 - ETA: 1s - loss: 0.0502 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0486 - accuracy: 0.98 - ETA: 1s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0464 - accuracy: 0.98 - ETA: 0s - loss: 0.0457 - accuracy: 0.98 - ETA: 0s - loss: 0.0453 - accuracy: 0.98 - ETA: 0s - loss: 0.0448 - accuracy: 0.98 - ETA: 0s - loss: 0.0441 - accuracy: 0.98 - ETA: 0s - loss: 0.0436 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0425 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0411 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0400 - accuracy: 0.98 - ETA: 0s - loss: 0.0396 - accuracy: 0.98 - 2s 346us/sample - loss: 0.0394 - accuracy: 0.9874 - val_loss: 0.2277 - val_accuracy: 0.9779\n",
      "Epoch 6/10\n",
      "4500/4500 [==============================] - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0255 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0230 - accuracy: 0.99 - ETA: 0s - loss: 0.0229 - accuracy: 0.99 - ETA: 0s - loss: 0.0227 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0223 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - 2s 355us/sample - loss: 0.0217 - accuracy: 0.9928 - val_loss: 0.2537 - val_accuracy: 0.9737\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - ETA: 1s - loss: 0.0125 - accuracy: 0.99 - ETA: 1s - loss: 0.0153 - accuracy: 0.99 - ETA: 1s - loss: 0.0146 - accuracy: 0.99 - ETA: 1s - loss: 0.0154 - accuracy: 0.99 - ETA: 1s - loss: 0.0152 - accuracy: 0.99 - ETA: 1s - loss: 0.0153 - accuracy: 0.99 - ETA: 1s - loss: 0.0152 - accuracy: 0.99 - ETA: 1s - loss: 0.0149 - accuracy: 0.99 - ETA: 0s - loss: 0.0147 - accuracy: 0.99 - ETA: 0s - loss: 0.0148 - accuracy: 0.99 - ETA: 0s - loss: 0.0150 - accuracy: 0.99 - ETA: 0s - loss: 0.0150 - accuracy: 0.99 - ETA: 0s - loss: 0.0150 - accuracy: 0.99 - ETA: 0s - loss: 0.0149 - accuracy: 0.99 - ETA: 0s - loss: 0.0148 - accuracy: 0.99 - ETA: 0s - loss: 0.0148 - accuracy: 0.99 - ETA: 0s - loss: 0.0148 - accuracy: 0.99 - ETA: 0s - loss: 0.0147 - accuracy: 0.99 - ETA: 0s - loss: 0.0146 - accuracy: 0.99 - ETA: 0s - loss: 0.0145 - accuracy: 0.99 - ETA: 0s - loss: 0.0145 - accuracy: 0.99 - ETA: 0s - loss: 0.0145 - accuracy: 0.99 - ETA: 0s - loss: 0.0144 - accuracy: 0.99 - ETA: 0s - loss: 0.0142 - accuracy: 0.99 - ETA: 0s - loss: 0.0141 - accuracy: 0.99 - ETA: 0s - loss: 0.0142 - accuracy: 0.99 - 1s 323us/sample - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.2750 - val_accuracy: 0.9721\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - ETA: 2s - loss: 0.0122 - accuracy: 0.99 - ETA: 1s - loss: 0.0109 - accuracy: 0.99 - ETA: 1s - loss: 0.0112 - accuracy: 0.99 - ETA: 1s - loss: 0.0113 - accuracy: 0.99 - ETA: 1s - loss: 0.0109 - accuracy: 0.99 - ETA: 1s - loss: 0.0108 - accuracy: 0.99 - ETA: 1s - loss: 0.0106 - accuracy: 0.99 - ETA: 1s - loss: 0.0110 - accuracy: 0.99 - ETA: 0s - loss: 0.0109 - accuracy: 0.99 - ETA: 0s - loss: 0.0106 - accuracy: 0.99 - ETA: 0s - loss: 0.0110 - accuracy: 0.99 - ETA: 0s - loss: 0.0110 - accuracy: 0.99 - ETA: 0s - loss: 0.0110 - accuracy: 0.99 - ETA: 0s - loss: 0.0109 - accuracy: 0.99 - ETA: 0s - loss: 0.0107 - accuracy: 0.99 - ETA: 0s - loss: 0.0108 - accuracy: 0.99 - ETA: 0s - loss: 0.0107 - accuracy: 0.99 - ETA: 0s - loss: 0.0106 - accuracy: 0.99 - ETA: 0s - loss: 0.0103 - accuracy: 0.99 - ETA: 0s - loss: 0.0104 - accuracy: 0.99 - ETA: 0s - loss: 0.0103 - accuracy: 0.99 - ETA: 0s - loss: 0.0103 - accuracy: 0.99 - ETA: 0s - loss: 0.0103 - accuracy: 0.99 - ETA: 0s - loss: 0.0102 - accuracy: 0.99 - ETA: 0s - loss: 0.0103 - accuracy: 0.99 - 1s 305us/sample - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.3265 - val_accuracy: 0.9698\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - ETA: 1s - loss: 0.0166 - accuracy: 0.99 - ETA: 1s - loss: 0.0085 - accuracy: 0.99 - ETA: 1s - loss: 0.0081 - accuracy: 0.99 - ETA: 1s - loss: 0.0076 - accuracy: 0.99 - ETA: 1s - loss: 0.0078 - accuracy: 0.99 - ETA: 0s - loss: 0.0081 - accuracy: 0.99 - ETA: 0s - loss: 0.0083 - accuracy: 0.99 - ETA: 0s - loss: 0.0080 - accuracy: 0.99 - ETA: 0s - loss: 0.0080 - accuracy: 0.99 - ETA: 0s - loss: 0.0081 - accuracy: 0.99 - ETA: 0s - loss: 0.0081 - accuracy: 0.99 - ETA: 0s - loss: 0.0079 - accuracy: 0.99 - ETA: 0s - loss: 0.0079 - accuracy: 0.99 - ETA: 0s - loss: 0.0079 - accuracy: 0.99 - ETA: 0s - loss: 0.0079 - accuracy: 0.99 - ETA: 0s - loss: 0.0080 - accuracy: 0.99 - ETA: 0s - loss: 0.0078 - accuracy: 0.99 - ETA: 0s - loss: 0.0077 - accuracy: 0.99 - ETA: 0s - loss: 0.0077 - accuracy: 0.99 - ETA: 0s - loss: 0.0078 - accuracy: 0.99 - ETA: 0s - loss: 0.0078 - accuracy: 0.99 - ETA: 0s - loss: 0.0077 - accuracy: 0.99 - ETA: 0s - loss: 0.0077 - accuracy: 0.99 - ETA: 0s - loss: 0.0078 - accuracy: 0.99 - ETA: 0s - loss: 0.0078 - accuracy: 0.99 - ETA: 0s - loss: 0.0077 - accuracy: 0.99 - ETA: 0s - loss: 0.0077 - accuracy: 0.99 - ETA: 0s - loss: 0.0077 - accuracy: 0.99 - 2s 357us/sample - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.3241 - val_accuracy: 0.9710\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500/4500 [==============================] - ETA: 1s - loss: 0.0088 - accuracy: 0.99 - ETA: 1s - loss: 0.0059 - accuracy: 0.99 - ETA: 1s - loss: 0.0057 - accuracy: 0.99 - ETA: 1s - loss: 0.0057 - accuracy: 0.99 - ETA: 1s - loss: 0.0057 - accuracy: 0.99 - ETA: 1s - loss: 0.0059 - accuracy: 0.99 - ETA: 1s - loss: 0.0061 - accuracy: 0.99 - ETA: 1s - loss: 0.0062 - accuracy: 0.99 - ETA: 1s - loss: 0.0063 - accuracy: 0.99 - ETA: 1s - loss: 0.0062 - accuracy: 0.99 - ETA: 0s - loss: 0.0062 - accuracy: 0.99 - ETA: 0s - loss: 0.0064 - accuracy: 0.99 - ETA: 0s - loss: 0.0064 - accuracy: 0.99 - ETA: 0s - loss: 0.0064 - accuracy: 0.99 - ETA: 0s - loss: 0.0064 - accuracy: 0.99 - ETA: 0s - loss: 0.0064 - accuracy: 0.99 - ETA: 0s - loss: 0.0063 - accuracy: 0.99 - ETA: 0s - loss: 0.0062 - accuracy: 0.99 - ETA: 0s - loss: 0.0061 - accuracy: 0.99 - ETA: 0s - loss: 0.0061 - accuracy: 0.99 - ETA: 0s - loss: 0.0062 - accuracy: 0.99 - ETA: 0s - loss: 0.0061 - accuracy: 0.99 - ETA: 0s - loss: 0.0062 - accuracy: 0.99 - ETA: 0s - loss: 0.0062 - accuracy: 0.99 - ETA: 0s - loss: 0.0062 - accuracy: 0.99 - ETA: 0s - loss: 0.0062 - accuracy: 0.99 - 1s 318us/sample - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.3417 - val_accuracy: 0.9698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1f784b50>"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_tr,epochs=10,validation_split=0.1,initial_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 153us/sample\n"
     ]
    }
   ],
   "source": [
    "y_test= model.predict(test_cnn_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metric(y_test,original_ans):\n",
    "    output_class_pred=[]\n",
    "    y_test=y_test.tolist()\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    for i in range(len(y_test)):\n",
    "        m=max(y_test[i])\n",
    "        output_class_pred.append([label_names[y_test[i].index(m)],m])\n",
    "        \n",
    "    for t in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]: \n",
    "        for i in range(len(y_test)):\n",
    "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
    "                output_class_pred[i][0]=\"oos\"\n",
    "        \n",
    "        rightly_predicted_count=0\n",
    "        for i in range(len(y_test)):\n",
    "            if( output_class_pred[i][0]==original_ans[i]):\n",
    "                rightly_predicted_count+=1\n",
    "\n",
    "        accuracy=rightly_predicted_count/len(y_test)\n",
    "        print('accuracy',rightly_predicted_count/len(y_test))\n",
    "        TP=0\n",
    "        FP=0\n",
    "        TN=0\n",
    "        FN=0\n",
    "        for i in range(len(y_test)):\n",
    "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
    "                FP+=1\n",
    "        \n",
    "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
    "                TP+=1\n",
    "        \n",
    "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
    "                FN+=1\n",
    "        \n",
    "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
    "                TN+=1\n",
    "        precision=TP/(TP+FP)\n",
    "        recalll=TP/(FN+TP)\n",
    "        F1=2*precision*recalll/(precision+recalll)\n",
    "        sensiti=TP/(TP+FN)\n",
    "        specifici=TN/(TN+FP)\n",
    "        numerator=TP*TN - FP*FN\n",
    "    \n",
    "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "        MCc=numerator/denominator\n",
    "        G_mean1=np.sqrt(sensiti*precision)\n",
    "        G_mean2=np.sqrt(sensiti*specifici)\n",
    "        print('precision_oos:' ,TP/(TP+FP))\n",
    "        print('recall_oos:',TP/(FN+TP))\n",
    "        print(\"F1_oos:\",F1)\n",
    "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
    "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
    "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "        print(\"MCC :\",MCc)\n",
    "        acc.append(accuracy)\n",
    "        pre.append(precision)\n",
    "        recall.append(recalll)\n",
    "        f1.append(F1)\n",
    "        specificity.append(specifici)\n",
    "        sensitivity.append(sensiti)\n",
    "        GMean1.append(G_mean1)\n",
    "        Gmean2.append(G_mean2)\n",
    "        MCC.append(MCc)\n",
    "        matrix=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    data={'threshold':matrix,'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.730625\n",
      "precision_oos: 0.2558139534883721\n",
      "recall_oos: 0.22\n",
      "F1_oos: 0.23655913978494625\n",
      "Specificity_oos: 0.9573333333333334\n",
      "Sensitivity_oos  0.22\n",
      "G-mean1: 0.2372321010475645\n",
      "G-mean2 0.458926283114547\n",
      "MCC : 0.19033727348336452\n",
      "accuracy 0.733125\n",
      "precision_oos: 0.26282051282051283\n",
      "recall_oos: 0.41\n",
      "F1_oos: 0.3203125\n",
      "Specificity_oos: 0.9233333333333333\n",
      "Sensitivity_oos  0.41\n",
      "G-mean1: 0.3282627153004286\n",
      "G-mean2 0.6152777150739873\n",
      "MCC : 0.2720059969061589\n",
      "accuracy 0.724375\n",
      "precision_oos: 0.23605150214592274\n",
      "recall_oos: 0.55\n",
      "F1_oos: 0.3303303303303303\n",
      "Specificity_oos: 0.8813333333333333\n",
      "Sensitivity_oos  0.55\n",
      "G-mean1: 0.36031698014423014\n",
      "G-mean2 0.6962279320260955\n",
      "MCC : 0.2960032185625604\n",
      "accuracy 0.72375\n",
      "precision_oos: 0.2422360248447205\n",
      "recall_oos: 0.78\n",
      "F1_oos: 0.36966824644549773\n",
      "Specificity_oos: 0.8373333333333334\n",
      "Sensitivity_oos  0.78\n",
      "G-mean1: 0.4346770058087752\n",
      "G-mean2 0.8081584003151857\n",
      "MCC : 0.3727108145509367\n",
      "accuracy 0.704375\n",
      "precision_oos: 0.21188630490956073\n",
      "recall_oos: 0.82\n",
      "F1_oos: 0.33675564681724846\n",
      "Specificity_oos: 0.7966666666666666\n",
      "Sensitivity_oos  0.82\n",
      "G-mean1: 0.4168294255757861\n",
      "G-mean2 0.8082491365084571\n",
      "MCC : 0.3485862242522961\n",
      "accuracy 0.690625\n",
      "precision_oos: 0.18981481481481483\n",
      "recall_oos: 0.82\n",
      "F1_oos: 0.30827067669172936\n",
      "Specificity_oos: 0.7666666666666667\n",
      "Sensitivity_oos  0.82\n",
      "G-mean1: 0.3945226839462443\n",
      "G-mean2 0.7928850273946827\n",
      "MCC : 0.3198701749162417\n",
      "accuracy 0.669375\n",
      "precision_oos: 0.1721311475409836\n",
      "recall_oos: 0.84\n",
      "F1_oos: 0.2857142857142857\n",
      "Specificity_oos: 0.7306666666666667\n",
      "Sensitivity_oos  0.84\n",
      "G-mean1: 0.3802501333785778\n",
      "G-mean2 0.7834283630300859\n",
      "MCC : 0.3000307934557074\n",
      "accuracy 0.636875\n",
      "precision_oos: 0.15263157894736842\n",
      "recall_oos: 0.87\n",
      "F1_oos: 0.2597014925373135\n",
      "Specificity_oos: 0.678\n",
      "Sensitivity_oos  0.87\n",
      "G-mean1: 0.36440290021377514\n",
      "G-mean2 0.7680234371423831\n",
      "MCC : 0.27699386348579813\n",
      "accuracy 0.58625\n",
      "precision_oos: 0.1378809869375907\n",
      "recall_oos: 0.95\n",
      "F1_oos: 0.24081115335868186\n",
      "Specificity_oos: 0.604\n",
      "Sensitivity_oos  0.95\n",
      "G-mean1: 0.3619211759357432\n",
      "G-mean2 0.757495874576225\n",
      "MCC : 0.2708236549016987\n"
     ]
    }
   ],
   "source": [
    "res=check_metric(y_test,original_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_all</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>specificity</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>Gmean1</th>\n",
       "      <th>Gmean2</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.730625</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.237232</td>\n",
       "      <td>0.458926</td>\n",
       "      <td>0.190337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.733125</td>\n",
       "      <td>0.262821</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.328263</td>\n",
       "      <td>0.615278</td>\n",
       "      <td>0.272006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.724375</td>\n",
       "      <td>0.236052</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.330330</td>\n",
       "      <td>0.881333</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.360317</td>\n",
       "      <td>0.696228</td>\n",
       "      <td>0.296003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.723750</td>\n",
       "      <td>0.242236</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.369668</td>\n",
       "      <td>0.837333</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.434677</td>\n",
       "      <td>0.808158</td>\n",
       "      <td>0.372711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.704375</td>\n",
       "      <td>0.211886</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.336756</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.416829</td>\n",
       "      <td>0.808249</td>\n",
       "      <td>0.348586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.690625</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.308271</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.394523</td>\n",
       "      <td>0.792885</td>\n",
       "      <td>0.319870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.669375</td>\n",
       "      <td>0.172131</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.730667</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.380250</td>\n",
       "      <td>0.783428</td>\n",
       "      <td>0.300031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.636875</td>\n",
       "      <td>0.152632</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.259701</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.364403</td>\n",
       "      <td>0.768023</td>\n",
       "      <td>0.276994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.586250</td>\n",
       "      <td>0.137881</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.240811</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.361921</td>\n",
       "      <td>0.757496</td>\n",
       "      <td>0.270824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_all  precision  recall  F1_score  specificity  \\\n",
       "0        0.1      0.730625   0.255814    0.22  0.236559     0.957333   \n",
       "1        0.2      0.733125   0.262821    0.41  0.320312     0.923333   \n",
       "2        0.3      0.724375   0.236052    0.55  0.330330     0.881333   \n",
       "3        0.4      0.723750   0.242236    0.78  0.369668     0.837333   \n",
       "4        0.5      0.704375   0.211886    0.82  0.336756     0.796667   \n",
       "5        0.6      0.690625   0.189815    0.82  0.308271     0.766667   \n",
       "6        0.7      0.669375   0.172131    0.84  0.285714     0.730667   \n",
       "7        0.8      0.636875   0.152632    0.87  0.259701     0.678000   \n",
       "8        0.9      0.586250   0.137881    0.95  0.240811     0.604000   \n",
       "\n",
       "   sensitivity    Gmean1    Gmean2       MCC  \n",
       "0         0.22  0.237232  0.458926  0.190337  \n",
       "1         0.41  0.328263  0.615278  0.272006  \n",
       "2         0.55  0.360317  0.696228  0.296003  \n",
       "3         0.78  0.434677  0.808158  0.372711  \n",
       "4         0.82  0.416829  0.808249  0.348586  \n",
       "5         0.82  0.394523  0.792885  0.319870  \n",
       "6         0.84  0.380250  0.783428  0.300031  \n",
       "7         0.87  0.364403  0.768023  0.276994  \n",
       "8         0.95  0.361921  0.757496  0.270824  "
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('results.csv', mode='w', index = False, header=res.columns,columns=res.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
